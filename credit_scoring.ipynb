{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIKbYKrWA/taPOvSCIvq3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasiCode/credit-scoring/blob/main/credit_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим задачу кредитного скоринга на основе кредитной истории клиента. Мы располагаем огромным датасетом - более полутора миллионов записей с кредитной историей анонимизированных клиентов Альфа Банка. На основе кредитной истории клиента до момента подачи заявки на новый кредит нужно оценить, насколько благонадежным является клиент, и определить вероятность его ухода в дефолт по новому кредиту, то есть предсказывать, насколько вероятна невыплата кредита со стороны потенциального клиента банка. Каждый кредит описывается набором из 60 категориальных признаков.\n",
        "\n",
        "В этом решении мы будем использовать градиентный бустинг на основе LightGBM для регрессии значения, равного вероятности ухода клиента в дефолт."
      ],
      "metadata": {
        "id": "pfw92CAADw7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет представляет собой набор директорий с parquet файлами. Этот бинарный формат крайне эффективно сжимает данные по колонкам. Однако, для непосредственной работы с данными и построения моделей нам нужно прочитать их и трансформировать в pandas.DataFrame. При этом сделать это эффективно по памяти."
      ],
      "metadata": {
        "id": "XioTSwAWEvf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем модуль fastparquet - он будет использоваться для быстрого разархивирования parquet файлов в csv."
      ],
      "metadata": {
        "id": "0xYLMV02jnmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install fastparquet"
      ],
      "metadata": {
        "id": "jYP1fP9BM4zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортим необходимые модули:"
      ],
      "metadata": {
        "id": "ad9CLZe9jxF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if not os.environ.get('KAGGLE_USERNAME'):\n",
        "  os.environ['KAGGLE_USERNAME'] = getpass.getpass('Enter username for Kaggle: ')\n",
        "\n",
        "if not os.environ.get('KAGGLE_KEY'):\n",
        "  os.environ['KAGGLE_KEY'] = getpass.getpass('Enter API key for Kaggle: ')"
      ],
      "metadata": {
        "id": "MbQpuCz9kl-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from fastparquet import ParquetFile\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "metadata": {
        "id": "8BHwFNc0MmWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подтягиваем датасет с Kaggle:"
      ],
      "metadata": {
        "id": "u3lU-NuSkmn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "\n",
        "competition_name = 'alfa-bank-pd-credit-history'\n",
        "download_path = './kaggle_data'\n",
        "\n",
        "if not os.path.exists(download_path):\n",
        "    os.makedirs(download_path)\n",
        "\n",
        "try:\n",
        "    api.competition_download_files(competition_name, path=download_path, quiet=False)\n",
        "    print(f'Competition files downloaded to: {download_path}')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'Error downloading competition data: {e}')\n",
        "    print('Check the competition name and your Kaggle API credentials.')\n",
        "\n",
        "\n",
        "files = os.listdir(download_path)\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith('.zip'):\n",
        "        zip_path = os.path.join(download_path, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(download_path)\n",
        "            print(f'Extracted: {file}')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error extracting {file}: {e}')\n",
        "\n",
        "print('Download and extraction complete.')"
      ],
      "metadata": {
        "id": "STyH2Rcgkro1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "При чтении всех данных сразу, они займут значительный объем памяти (более 11 GB). Решение – читать данные итеративно небольшими чанками. Чанки организованы таким образом, что для конкретного клиента вся информация о его кредитной истории до момента подачи заявки на кредит расположена внутри одного чанка. Это позволяет загружать данные в память небольшими порциями, выделять все необходимые признаки и получать результирующий фрейм для моделирования. Для этих целей объявим функцию read_parquet_from_local."
      ],
      "metadata": {
        "id": "Bqut_2hYj2JB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_etPEy5RHR0l"
      },
      "outputs": [],
      "source": [
        "def read_parquet_from_local(\n",
        "        path: str, start_from: int = 0,\n",
        "        num_parts_to_read: int = 2, columns: List[str] = None,\n",
        "        verbose: bool = False) -> pd.DataFrame:\n",
        "\n",
        "    res = []\n",
        "    start_from = max(0, start_from)\n",
        "    dataset_paths = {\n",
        "        int(os.path.splitext(filename)[0].split(\"_\")[-1]):\n",
        "            os.path.join(path, filename)\n",
        "            for filename in os.listdir(path)\n",
        "    }\n",
        "    chunks = [dataset_paths[num] for num in sorted(\n",
        "        dataset_paths.keys()) if num >= start_from][:num_parts_to_read]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Reading chunks:\", *chunks, sep=\"\\n\")\n",
        "\n",
        "    for chunk_path in tqdm(chunks, desc=\"Reading dataset with Pandas\"):\n",
        "        pf = ParquetFile(chunk_path)\n",
        "        chunk = pf.to_pandas(columns)\n",
        "        res.append(chunk)\n",
        "\n",
        "    return pd.concat(res).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведем тест функции и оценим занимаемую память:"
      ],
      "metadata": {
        "id": "kHggc7mVGTYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = read_parquet_from_local('./kaggle_data/data_for_competition/train_data', start_from=0, num_parts_to_read=1)\n",
        "\n",
        "memory_usage_of_frame = data_frame.memory_usage(index=True).sum() / 10**9\n",
        "expected_memory_usage = memory_usage_of_frame * 12\n",
        "print(f\"Объем памяти в RAM одной партиции данных с кредитными историями: {round(memory_usage_of_frame, 3)} GB\")\n",
        "print(f\"Ожидаемый размер в RAM всего датасета: {round(expected_memory_usage, 3)} GB\")"
      ],
      "metadata": {
        "id": "Qtsz7wK90sl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все фичи нашего датафрейма являются категориальными. Выведем для каждой фичи количество ее уникальных значений."
      ],
      "metadata": {
        "id": "IUUTcg9XGdR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feat, count in zip(data_frame.columns.values, data_frame.nunique()):\n",
        "    print(f'{feat}: {count}')"
      ],
      "metadata": {
        "id": "27SPg4ji0e-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что 12 фичей имеют более 10 уникальных значений. Если кодировать их через OneHotEncoding, это приведет к \"взрыву\" необходимого объема памяти, так как каждая из 12 фичей будет порождать N_i столбцов, где N_i - количество уникальных значений у i-й фичи."
      ],
      "metadata": {
        "id": "yeQJQtLPG1Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почистим память:"
      ],
      "metadata": {
        "id": "WD-pJEBBHRK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del data_frame\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "rTNj34DWc-em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Базовым подходом к решению этой задачи является построение классической модели машинного обучения на аггрегациях от последовательностей категориальных признаков. В данном случае мы закодируем признаки с помощью count-encoding'а, применим к ним аггрегирование (наиболее очевидными аггрегациями являются среднее и сумма) и обучим на этом градиентный бустинг из реализации lightgbm.\n",
        "\n",
        "Описанный подход к обработке кредитных историй клиентов реализован в виде класса-трансформера DataAggregator ниже:"
      ],
      "metadata": {
        "id": "prg1ArSLkN-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "class DataAggregator(object):\n",
        "    def __init__(self):\n",
        "        self.encoded_feats = None\n",
        "\n",
        "    def __extract_count_aggregations(\n",
        "            self,\n",
        "            dataframe: pd.DataFrame\n",
        "        ) -> pd.DataFrame:\n",
        "        feat_cols = list(dataframe.columns.values)\n",
        "        feat_cols.remove('id')\n",
        "        feat_cols.remove('rn')\n",
        "\n",
        "        encoded_feats = dataframe[feat_cols].apply(\n",
        "            lambda col: col.map(col.value_counts())\n",
        "        )\n",
        "        encoded_feats['id'] = dataframe['id']\n",
        "\n",
        "        feats = encoded_feats.groupby('id').agg(['mean', 'sum']).reset_index()\n",
        "        feats.columns = ['_'.join(col) if col[1] else col[0] for col in feats.columns.values]\n",
        "\n",
        "        return feats\n",
        "\n",
        "    def __transform_data(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 25,\n",
        "            mode: str = 'fit_transform',\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        assert mode in ['fit_transform', 'transform'\n",
        "            ],f'Unrecognized mode: {mode}. Available modes: fit_transform, transform'\n",
        "\n",
        "        preprocessed_frames = []\n",
        "\n",
        "        for step in tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
        "                         desc='Transforming sequential data'):\n",
        "            dataframe = read_parquet_from_local(\n",
        "                path_to_dataset,\n",
        "                start_from=step,\n",
        "                num_parts_to_read=num_parts_to_preprocess_at_once,\n",
        "                verbose=verbose\n",
        "            )\n",
        "            feats = self.__extract_count_aggregations(dataframe)\n",
        "            if save_to_path:\n",
        "                feats.to_parquet(os.path.join(\n",
        "                    save_to_path, f'processed_chunk_{step}.pq'\n",
        "                ))\n",
        "                preprocessed_frames.append(feats)\n",
        "\n",
        "        feats = pd.concat(preprocessed_frames)\n",
        "        feats.fillna(np.uint8(0), inplace=True)\n",
        "        dummies = list(feats.columns.values)\n",
        "        dummies.remove('id')\n",
        "\n",
        "        if (mode == 'fit_transform'):\n",
        "            self.encoded_feats = dummies\n",
        "        else:\n",
        "            assert not self.encoded_feats is None, 'Transformer not fitted'\n",
        "            for col in self.encoded_feats:\n",
        "                if not col in dummies:\n",
        "                    feats[col] = np.uint8(0)\n",
        "\n",
        "        return feats[['id']+self.encoded_feats]\n",
        "\n",
        "    def fit_transform(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 50,\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        return self.__transform_data(\n",
        "            path_to_dataset=path_to_dataset,\n",
        "            num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "            num_parts_total=num_parts_total,\n",
        "            mode='fit_transform',\n",
        "            save_to_path=save_to_path,\n",
        "            verbose=verbose)\n",
        "\n",
        "    def transform(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 50,\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        return self.__transform_data(\n",
        "            path_to_dataset=path_to_dataset,\n",
        "            num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "            num_parts_total=num_parts_total,\n",
        "            mode='transform',\n",
        "            save_to_path=save_to_path,\n",
        "            verbose=verbose)"
      ],
      "metadata": {
        "id": "bAGAAAbhHZf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим директорию для предобработанных признаков обучающей выборки:"
      ],
      "metadata": {
        "id": "CspSUtrFILvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./kaggle_data/data_for_competition/train_features"
      ],
      "metadata": {
        "id": "h_p6VNVU4dLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заполним эту директорию агрегированными данными:"
      ],
      "metadata": {
        "id": "jQVOaNbSIXp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "aggregator = DataAggregator()\n",
        "train_data = aggregator.fit_transform(\n",
        "    './kaggle_data/data_for_competition/train_data',\n",
        "    num_parts_to_preprocess_at_once=1,\n",
        "    num_parts_total=12,\n",
        "    save_to_path='./kaggle_data/data_for_competition/train_features',\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "vC_TdH2frSr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим типы данных полученного датафрейма (Заметим также, что он один занимает около 3 GB памяти):"
      ],
      "metadata": {
        "id": "KS-6vTxAJsXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes"
      ],
      "metadata": {
        "id": "_A9PyCzdNwyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вес датафрейма можно уменьшить вдвое безо всякой потери качества, переведя 64-битные данные внутри в 32-битные. Точность чисел с плавающей запятой и размер целочисленных значений внутри датафрейма умещаются в 32-битный формат данных целиком."
      ],
      "metadata": {
        "id": "yu2drx_KJ7Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.memory_usage(deep=True)"
      ],
      "metadata": {
        "id": "RIlL3lfmKIVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_data.select_dtypes(include=['int64']).columns:\n",
        "    train_data[col] = pd.to_numeric(train_data[col], downcast='integer')\n",
        "\n",
        "for col in train_data.select_dtypes(include=['float64']).columns:\n",
        "    train_data[col] = train_data[col].astype(np.float32)"
      ],
      "metadata": {
        "id": "l9YuPnTZO_dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes"
      ],
      "metadata": {
        "id": "J4JiON__QLr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.memory_usage(deep=True)"
      ],
      "metadata": {
        "id": "TADxwiFlQQiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим теперь механизм кросс-валидации. Учитывая, что мы загружаем данные отдельными партациями, нам не подходит \"классический\" алгоритм разбиения датасета на train\\val выборки.\n",
        "\n",
        "Дело в том, что при разбиении каждой загружаемой партации на фолды \"на месте\" мы теряем репрезентативность, так как out-of-fold часть выборки будет локальной, а для адекватной валидации нужно, чтобы OOF охватывала весь датасет.\n",
        "\n",
        "Более того, без механизма согласования OOF по фолдам мы не сможем собрать аггрегированное предсказание наших моделей на тесте. Это связано с архитектурой решения и будет видно позже.\n",
        "\n",
        "Итак, рассмотрим теперь следующий механизм кросс-валидации:\n",
        "1. Создается датафрейм, содержащий id объектов всей (!) выборки. Учитывая, что это единственный int столбец, он не займет критически много места;\n",
        "2. Этот датафрейм расширяется колонкой fold, которая будет указывать, к валидации какого по счету фолда относится элемент;\n",
        "3. Колонка fold заполняется с использованием инструмента KFold из scikit_learn. Это стандартный инструмент для разбиения наборов данных на фолды;\n",
        "\n",
        "На этом этапе мы получаем датафрейм, глобально делящий все объекты из всех партаций в совокупности на фолды для кросс-валидации. Учитывая огромный объем каждой отдельной партации и случайность разбиения на фолды, мы получим примерно одинаковое соотношение объемов train и val частей в индивидуальных партациях.\n",
        "\n",
        "При обучении модели каждая партация загружается и рассматривается независимо от остальных. Используя полученный на этапе 3 датафрейм, из партации выбираются соответствующие текущему фолду строки. Для итогового случая создается отдельная модель. Она обучается на финальных данных и добавляется в общий ансамбль. Это гарантирует, что все модели ансамбля в совокупности будут обучены на полном объеме данных без исключения.  \n",
        "\n",
        "К концу обучения общее количество моделей в ансамбле будет равно MxN, по M моделей на каждую партацию, рассмотренную в N разных вариациях разбиения.\n",
        "\n",
        "Итоговое предсказание на тесте будет получено аггрегированием предсказаний всего ансамбля моделей. Именно на этом месте \"стреляет\" согласованность моделей в ансамбле."
      ],
      "metadata": {
        "id": "7_E0FDm4KXiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = {\n",
        "        os.path.join('./kaggle_data/data_for_competition/train_features', filename)\n",
        "        for filename in os.listdir('./kaggle_data/data_for_competition/train_features')\n",
        "}\n",
        "\n",
        "id_dfs = []\n",
        "for path in train_paths:\n",
        "    pf = ParquetFile(path)\n",
        "    df = pf.to_pandas(columns=['id'])\n",
        "    id_dfs.append(df)\n",
        "\n",
        "kf_ids = pd.concat(id_dfs, ignore_index=True)\n",
        "kf_ids['fold'] = -1\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=100, shuffle=True)\n",
        "for fold, (_, val_idx) in enumerate(kf.split(kf_ids)):\n",
        "    kf_ids.loc[val_idx, 'fold'] = fold"
      ],
      "metadata": {
        "id": "7XOh7fW9h5QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf_ids.fold.value_counts()"
      ],
      "metadata": {
        "id": "aJvMUxMDKXvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cols = list(train_data.columns.values)\n",
        "feat_cols.remove('id')"
      ],
      "metadata": {
        "id": "L-lleMFDK3am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cols"
      ],
      "metadata": {
        "id": "E3YgCdUvLC30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "vKttr7CKZTEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подгружаем в память целевую переменную из train_target.csv:"
      ],
      "metadata": {
        "id": "i7w6SOfsIuvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_target = pd.read_csv('./kaggle_data/data_for_competition/train_target.csv')"
      ],
      "metadata": {
        "id": "bfLvn73m1IRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "models = []\n",
        "\n",
        "tree_parameters = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 5,\n",
        "    'reg_lambda': 1,\n",
        "    'num_leaves': 64,\n",
        "    'n_estimators': 2000,\n",
        "    'min_data_in_leaf': 256,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "for fold_ in range(5):\n",
        "    print(f'Training with fold {fold_} started')\n",
        "\n",
        "    val_preds_list = []\n",
        "    val_targets_list = []\n",
        "\n",
        "    for path in train_paths:\n",
        "        pf = ParquetFile(path)\n",
        "        df = pf.to_pandas()\n",
        "        df = df.merge(kf_ids, on='id')\n",
        "        df = df.merge(train_target, on='id')\n",
        "\n",
        "        val = df[df['fold'] == fold]\n",
        "        train = df[df['fold'] != fold]\n",
        "\n",
        "        if val.empty or train.empty:\n",
        "            print('Skipped: no val or train data for this fold')\n",
        "            continue\n",
        "\n",
        "        features = [c for c in train.columns if c not in ('id', 'flag', 'fold')]\n",
        "\n",
        "        lgb_model = lgb.LGBMClassifier(**tree_parameters)\n",
        "        lgb_model.fit(\n",
        "            train[features], train.flag.values,\n",
        "            eval_set=[(val[features], val.flag.values)]\n",
        "        )\n",
        "\n",
        "        preds = lgb_model.predict_proba(val[features])[:, 1]\n",
        "        val_preds_list.append(preds)\n",
        "        val_targets_list.append(val['flag'].values)\n",
        "        models.append(lgb_model)\n",
        "\n",
        "        del df, val, train\n",
        "        gc.collect()\n",
        "\n",
        "    fold_preds = np.concatenate(val_preds_list)\n",
        "    fold_targets = np.concatenate(val_targets_list)\n",
        "    print('CV ROC-AUC: ', roc_auc_score(fold_targets, fold_preds))\n",
        "\n",
        "    print(f'Training with fold {fold_} completed')"
      ],
      "metadata": {
        "id": "aP_HGWrJHmZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь проделаем то же самое для тестовой выборки:"
      ],
      "metadata": {
        "id": "BV-4LQ6IIqc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./kaggle_data/data_for_competition/test_features"
      ],
      "metadata": {
        "id": "8lV3r7xL5oA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = aggregator.transform(\n",
        "    './kaggle_data/data_for_competition/test_data',\n",
        "    num_parts_to_preprocess_at_once=2,\n",
        "    num_parts_total=2,\n",
        "    save_to_path='./kaggle_data/data_for_competition/test_features',\n",
        "    verbose=True)\n",
        "\n",
        "for col in test_data.select_dtypes(include=['int64']).columns:\n",
        "    test_data[col] = pd.to_numeric(test_data[col], downcast='integer')\n",
        "\n",
        "for col in test_data.select_dtypes(include=['float64']).columns:\n",
        "    test_data[col] = test_data[col].astype(np.float32)"
      ],
      "metadata": {
        "id": "3EojSfui5nRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target = pd.read_csv('./kaggle_data/data_for_competition/test_target.csv')\n",
        "\n",
        "score = np.zeros(len(test_data))\n",
        "for model in tqdm(models):\n",
        "    score += model.predict_proba(test_data[feat_cols])[:, 1]/len(models)\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'].values,\n",
        "    'score': score\n",
        "})\n",
        "\n",
        "print('TEST ROC-AUC: ', roc_auc_score(test_target, score))"
      ],
      "metadata": {
        "id": "SYtybXw0Xq6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}