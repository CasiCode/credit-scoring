{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg8kgMrjvdt1S+Ub8LkAwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasiCode/credit-scoring/blob/main/credit_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим задачу кредитного скоринга на основе кредитной истории клиента. Мы располагаем огромным датасетом - более полутора миллионов записей с кредитной историей анонимизированных клиентов Альфа Банка. На основе кредитной истории клиента до момента подачи заявки на новый кредит нужно оценить, насколько благонадежным является клиент, и определить вероятность его ухода в дефолт по новому кредиту, то есть предсказывать, насколько вероятна невыплата кредита со стороны потенциального клиента банка. Каждый кредит описывается набором из 60 категориальных признаков.\n",
        "\n",
        "В этом решении мы будем использовать градиентный бустинг на основе LightGBM для регрессии значения, равного вероятности ухода клиента в дефолт."
      ],
      "metadata": {
        "id": "pfw92CAADw7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет представляет собой набор директорий с parquet файлами. Этот бинарный формат крайне эффективно сжимает данные по колонкам. Однако, для непосредственной работы с данными и построения моделей нам нужно прочитать их и трансформировать в pandas.DataFrame. При этом сделать это эффективно по памяти."
      ],
      "metadata": {
        "id": "XioTSwAWEvf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем модуль fastparquet - он будет использоваться для быстрого разархивирования parquet файлов в csv."
      ],
      "metadata": {
        "id": "0xYLMV02jnmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install fastparquet"
      ],
      "metadata": {
        "id": "jYP1fP9BM4zI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортим необходимые модули:"
      ],
      "metadata": {
        "id": "ad9CLZe9jxF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if not os.environ.get('KAGGLE_USERNAME'):\n",
        "  os.environ['KAGGLE_USERNAME'] = getpass.getpass('Enter username for Kaggle: ')\n",
        "\n",
        "if not os.environ.get('KAGGLE_KEY'):\n",
        "  os.environ['KAGGLE_KEY'] = getpass.getpass('Enter API key for Kaggle: ')"
      ],
      "metadata": {
        "id": "MbQpuCz9kl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6921fec-4667-46cd-f76e-00c09292b659"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter username for Kaggle: ··········\n",
            "Enter API key for Kaggle: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from fastparquet import ParquetFile\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "metadata": {
        "id": "8BHwFNc0MmWx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подтягиваем датасет с Kaggle:"
      ],
      "metadata": {
        "id": "u3lU-NuSkmn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "\n",
        "competition_name = 'alfa-bank-pd-credit-history'\n",
        "download_path = './kaggle_data'\n",
        "\n",
        "if not os.path.exists(download_path):\n",
        "    os.makedirs(download_path)\n",
        "\n",
        "try:\n",
        "    api.competition_download_files(competition_name, path=download_path, quiet=False)\n",
        "    print(f'Competition files downloaded to: {download_path}')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'Error downloading competition data: {e}')\n",
        "    print('Check the competition name and your Kaggle API credentials.')\n",
        "\n",
        "\n",
        "files = os.listdir(download_path)\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith('.zip'):\n",
        "        zip_path = os.path.join(download_path, file)\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(download_path)\n",
        "            print(f'Extracted: {file}')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Error extracting {file}: {e}')\n",
        "\n",
        "print('Download and extraction complete.')"
      ],
      "metadata": {
        "id": "STyH2Rcgkro1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e80a0d-1d8e-475c-8674-be0f4047a4d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alfa-bank-pd-credit-history.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Competition files downloaded to: ./kaggle_data\n",
            "Extracted: alfa-bank-pd-credit-history.zip\n",
            "Download and extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При чтении всех данных сразу, они займут значительный объем памяти (более 11 GB). Решение – читать данные итеративно небольшими чанками. Чанки организованы таким образом, что для конкретного клиента вся информация о его кредитной истории до момента подачи заявки на кредит расположена внутри одного чанка. Это позволяет загружать данные в память небольшими порциями, выделять все необходимые признаки и получать результирующий фрейм для моделирования. Для этих целей объявим функцию read_parquet_from_local."
      ],
      "metadata": {
        "id": "Bqut_2hYj2JB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_etPEy5RHR0l"
      },
      "outputs": [],
      "source": [
        "def read_parquet_from_local(\n",
        "        path: str, start_from: int = 0,\n",
        "        num_parts_to_read: int = 2, columns: List[str] = None,\n",
        "        verbose: bool = False) -> pd.DataFrame:\n",
        "\n",
        "    res = []\n",
        "    start_from = max(0, start_from)\n",
        "    dataset_paths = {\n",
        "        int(os.path.splitext(filename)[0].split(\"_\")[-1]):\n",
        "            os.path.join(path, filename)\n",
        "            for filename in os.listdir(path)\n",
        "    }\n",
        "    chunks = [dataset_paths[num] for num in sorted(\n",
        "        dataset_paths.keys()) if num >= start_from][:num_parts_to_read]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Reading chunks:\", *chunks, sep=\"\\n\")\n",
        "\n",
        "    for chunk_path in tqdm(chunks, desc=\"Reading dataset with Pandas\"):\n",
        "        pf = ParquetFile(chunk_path)\n",
        "        chunk = pf.to_pandas(columns)\n",
        "        res.append(chunk)\n",
        "\n",
        "    return pd.concat(res).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведем тест функции и оценим занимаемую память:"
      ],
      "metadata": {
        "id": "kHggc7mVGTYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = read_parquet_from_local('./kaggle_data/data_for_competition/train_data', start_from=0, num_parts_to_read=1)\n",
        "\n",
        "memory_usage_of_frame = data_frame.memory_usage(index=True).sum() / 10**9\n",
        "expected_memory_usage = memory_usage_of_frame * 12\n",
        "print(f\"Объем памяти в RAM одной партиции данных с кредитными историями: {round(memory_usage_of_frame, 3)} GB\")\n",
        "print(f\"Ожидаемый размер в RAM всего датасета: {round(expected_memory_usage, 3)} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtsz7wK90sl9",
        "outputId": "f5d98c92-d5b9-41d1-ec7e-be48f6d8a7f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Объем памяти в RAM одной партиции данных с кредитными историями: 0.964 GB\n",
            "Ожидаемый размер в RAM всего датасета: 11.564 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все фичи нашего датафрейма являются категориальными. Выведем для каждой фичи количество ее уникальных значений."
      ],
      "metadata": {
        "id": "IUUTcg9XGdR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feat, count in zip(data_frame.columns.values, data_frame.nunique()):\n",
        "    print(f'{feat}: {count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27SPg4ji0e-K",
        "outputId": "d00273c6-322f-4f14-9145-17a9bfd2d11b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: 250000\n",
            "rn: 51\n",
            "pre_since_opened: 20\n",
            "pre_since_confirmed: 18\n",
            "pre_pterm: 18\n",
            "pre_fterm: 17\n",
            "pre_till_pclose: 17\n",
            "pre_till_fclose: 16\n",
            "pre_loans_credit_limit: 20\n",
            "pre_loans_next_pay_summ: 7\n",
            "pre_loans_outstanding: 5\n",
            "pre_loans_total_overdue: 1\n",
            "pre_loans_max_overdue_sum: 3\n",
            "pre_loans_credit_cost_rate: 14\n",
            "pre_loans5: 8\n",
            "pre_loans530: 15\n",
            "pre_loans3060: 5\n",
            "pre_loans6090: 3\n",
            "pre_loans90: 4\n",
            "is_zero_loans5: 2\n",
            "is_zero_loans530: 2\n",
            "is_zero_loans3060: 2\n",
            "is_zero_loans6090: 2\n",
            "is_zero_loans90: 2\n",
            "pre_util: 20\n",
            "pre_over2limit: 20\n",
            "pre_maxover2limit: 20\n",
            "is_zero_util: 2\n",
            "is_zero_over2limit: 2\n",
            "is_zero_maxover2limit: 2\n",
            "enc_paym_0: 4\n",
            "enc_paym_1: 4\n",
            "enc_paym_2: 4\n",
            "enc_paym_3: 4\n",
            "enc_paym_4: 4\n",
            "enc_paym_5: 4\n",
            "enc_paym_6: 4\n",
            "enc_paym_7: 4\n",
            "enc_paym_8: 4\n",
            "enc_paym_9: 4\n",
            "enc_paym_10: 4\n",
            "enc_paym_11: 4\n",
            "enc_paym_12: 4\n",
            "enc_paym_13: 4\n",
            "enc_paym_14: 4\n",
            "enc_paym_15: 4\n",
            "enc_paym_16: 4\n",
            "enc_paym_17: 4\n",
            "enc_paym_18: 4\n",
            "enc_paym_19: 4\n",
            "enc_paym_20: 4\n",
            "enc_paym_21: 4\n",
            "enc_paym_22: 4\n",
            "enc_paym_23: 4\n",
            "enc_paym_24: 4\n",
            "enc_loans_account_holder_type: 7\n",
            "enc_loans_credit_status: 7\n",
            "enc_loans_credit_type: 6\n",
            "enc_loans_account_cur: 4\n",
            "pclose_flag: 2\n",
            "fclose_flag: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что 12 фичей имеют более 10 уникальных значений. Если кодировать их через OneHotEncoding, это приведет к \"взрыву\" необходимого объема памяти, так как каждая из 12 фичей будет порождать N_i столбцов, где N_i - количество уникальных значений у i-й фичи."
      ],
      "metadata": {
        "id": "yeQJQtLPG1Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Почистим память:"
      ],
      "metadata": {
        "id": "WD-pJEBBHRK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del data_frame\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTNj34DWc-em",
        "outputId": "70d5d21e-dd9e-41fd-b3d8-76cbe0a1aac7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Базовым подходом к решению этой задачи является построение классической модели машинного обучения на аггрегациях от последовательностей категориальных признаков. В данном случае мы закодируем признаки с помощью count-encoding'а, применим к ним аггрегирование (наиболее очевидными аггрегациями являются среднее и сумма) и обучим на этом градиентный бустинг из реализации lightgbm.\n",
        "\n",
        "Описанный подход к обработке кредитных историй клиентов реализован в виде класса-трансформера DataAggregator ниже:"
      ],
      "metadata": {
        "id": "prg1ArSLkN-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "class DataAggregator(object):\n",
        "    def __init__(self):\n",
        "        self.encoded_feats = None\n",
        "\n",
        "    def __extract_count_aggregations(\n",
        "            self,\n",
        "            dataframe: pd.DataFrame\n",
        "        ) -> pd.DataFrame:\n",
        "        feat_cols = list(dataframe.columns.values)\n",
        "        feat_cols.remove('id')\n",
        "        feat_cols.remove('rn')\n",
        "\n",
        "        encoded_feats = dataframe[feat_cols].apply(\n",
        "            lambda col: col.map(col.value_counts())\n",
        "        )\n",
        "        encoded_feats['id'] = dataframe['id']\n",
        "\n",
        "        feats = encoded_feats.groupby('id').agg(['mean', 'sum']).reset_index()\n",
        "        feats.columns = ['_'.join(col) if col[1] else col[0] for col in feats.columns.values]\n",
        "\n",
        "        return feats\n",
        "\n",
        "    def __transform_data(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 25,\n",
        "            mode: str = 'fit_transform',\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        assert mode in ['fit_transform', 'transform'\n",
        "            ],f'Unrecognized mode: {mode}. Available modes: fit_transform, transform'\n",
        "\n",
        "        preprocessed_frames = []\n",
        "\n",
        "        for step in tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
        "                         desc='Transforming sequential data'):\n",
        "            dataframe = read_parquet_from_local(\n",
        "                path_to_dataset,\n",
        "                start_from=step,\n",
        "                num_parts_to_read=num_parts_to_preprocess_at_once,\n",
        "                verbose=verbose\n",
        "            )\n",
        "            feats = self.__extract_count_aggregations(dataframe)\n",
        "            if save_to_path:\n",
        "                feats.to_parquet(os.path.join(\n",
        "                    save_to_path, f'processed_chunk_{step}.pq'\n",
        "                ))\n",
        "                preprocessed_frames.append(feats)\n",
        "\n",
        "        feats = pd.concat(preprocessed_frames)\n",
        "        feats.fillna(np.uint8(0), inplace=True)\n",
        "        dummies = list(feats.columns.values)\n",
        "        dummies.remove('id')\n",
        "\n",
        "        if (mode == 'fit_transform'):\n",
        "            self.encoded_feats = dummies\n",
        "        else:\n",
        "            assert not self.encoded_feats is None, 'Transformer not fitted'\n",
        "            for col in self.encoded_feats:\n",
        "                if not col in dummies:\n",
        "                    feats[col] = np.uint8(0)\n",
        "\n",
        "        return feats[['id']+self.encoded_feats]\n",
        "\n",
        "    def fit_transform(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 50,\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        return self.__transform_data(\n",
        "            path_to_dataset=path_to_dataset,\n",
        "            num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "            num_parts_total=num_parts_total,\n",
        "            mode='fit_transform',\n",
        "            save_to_path=save_to_path,\n",
        "            verbose=verbose)\n",
        "\n",
        "    def transform(\n",
        "            self,\n",
        "            path_to_dataset: str,\n",
        "            num_parts_to_preprocess_at_once: int = 1,\n",
        "            num_parts_total: int = 50,\n",
        "            save_to_path=None,\n",
        "            verbose: bool = False):\n",
        "        return self.__transform_data(\n",
        "            path_to_dataset=path_to_dataset,\n",
        "            num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
        "            num_parts_total=num_parts_total,\n",
        "            mode='transform',\n",
        "            save_to_path=save_to_path,\n",
        "            verbose=verbose)"
      ],
      "metadata": {
        "id": "bAGAAAbhHZf9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим директорию для предобработанных признаков обучающей выборки:"
      ],
      "metadata": {
        "id": "CspSUtrFILvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./kaggle_data/data_for_competition/train_features"
      ],
      "metadata": {
        "id": "h_p6VNVU4dLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac5f22e-067e-482f-bf11-9c73d6ee78e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./kaggle_data/data_for_competition/train_features’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заполним эту директорию агрегированными данными:"
      ],
      "metadata": {
        "id": "jQVOaNbSIXp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "aggregator = DataAggregator()\n",
        "train_data = aggregator.fit_transform(\n",
        "    './kaggle_data/data_for_competition/train_data',\n",
        "    num_parts_to_preprocess_at_once=1,\n",
        "    num_parts_total=12,\n",
        "    save_to_path='./kaggle_data/data_for_competition/train_features',\n",
        "    verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC_TdH2frSr4",
        "outputId": "e9b194af-e510-4822-f4db-0e8039ea95ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTransforming sequential data:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_0.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Transforming sequential data:   8%|▊         | 1/12 [00:09<01:46,  9.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_1.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "Transforming sequential data:  17%|█▋        | 2/12 [00:19<01:35,  9.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_2.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
            "Transforming sequential data:  25%|██▌       | 3/12 [00:29<01:29,  9.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_3.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
            "Transforming sequential data:  33%|███▎      | 4/12 [00:38<01:17,  9.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_4.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Transforming sequential data:  42%|████▏     | 5/12 [00:48<01:07,  9.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_5.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
            "Transforming sequential data:  50%|█████     | 6/12 [00:59<01:00, 10.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_6.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Transforming sequential data:  58%|█████▊    | 7/12 [01:08<00:49,  9.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_7.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
            "Transforming sequential data:  67%|██████▋   | 8/12 [01:17<00:38,  9.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_8.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
            "Transforming sequential data:  75%|███████▌  | 9/12 [01:28<00:29,  9.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_9.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
            "Transforming sequential data:  83%|████████▎ | 10/12 [01:38<00:19,  9.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_10.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
            "Transforming sequential data:  92%|█████████▏| 11/12 [01:48<00:09,  9.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading chunks:\n",
            "./kaggle_data/data_for_competition/train_data/train_data_11.pq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Reading dataset with Pandas:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Reading dataset with Pandas: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
            "Transforming sequential data: 100%|██████████| 12/12 [01:59<00:00,  9.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 37s, sys: 25.4 s, total: 2min 3s\n",
            "Wall time: 2min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим типы данных полученного датафрейма (Заметим также, что он один занимает около 3 GB памяти):"
      ],
      "metadata": {
        "id": "KS-6vTxAJsXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "_A9PyCzdNwyX",
        "outputId": "673f5030-c51a-4c8e-d997-ab8bf5e06a0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                             int64\n",
              "pre_since_opened_mean        float64\n",
              "pre_since_opened_sum           int64\n",
              "pre_since_confirmed_mean     float64\n",
              "pre_since_confirmed_sum        int64\n",
              "                              ...   \n",
              "enc_loans_account_cur_sum      int64\n",
              "pclose_flag_mean             float64\n",
              "pclose_flag_sum                int64\n",
              "fclose_flag_mean             float64\n",
              "fclose_flag_sum                int64\n",
              "Length: 119, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_mean</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_sum</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_mean</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_sum</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enc_loans_account_cur_sum</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_mean</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_sum</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_mean</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_sum</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>119 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вес датафрейма можно уменьшить вдвое безо всякой потери качества, переведя 64-битные данные внутри в 32-битные. Точность чисел с плавающей запятой и размер целочисленных значений внутри датафрейма умещаются в 32-битный формат данных целиком."
      ],
      "metadata": {
        "id": "yu2drx_KJ7Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.memory_usage(deep=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "RIlL3lfmKIVM",
        "outputId": "276db09b-55d5-4d6f-9d16-663da3e10337"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index                        24000000\n",
              "id                           24000000\n",
              "pre_since_opened_mean        24000000\n",
              "pre_since_opened_sum         24000000\n",
              "pre_since_confirmed_mean     24000000\n",
              "                               ...   \n",
              "enc_loans_account_cur_sum    24000000\n",
              "pclose_flag_mean             24000000\n",
              "pclose_flag_sum              24000000\n",
              "fclose_flag_mean             24000000\n",
              "fclose_flag_sum              24000000\n",
              "Length: 120, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Index</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_mean</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_sum</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_mean</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enc_loans_account_cur_sum</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_mean</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_sum</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_mean</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_sum</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_data.select_dtypes(include=['int64']).columns:\n",
        "    train_data[col] = pd.to_numeric(train_data[col], downcast='integer')\n",
        "\n",
        "for col in train_data.select_dtypes(include=['float64']).columns:\n",
        "    train_data[col] = train_data[col].astype(np.float32)"
      ],
      "metadata": {
        "id": "l9YuPnTZO_dZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "J4JiON__QLr4",
        "outputId": "967bc88e-6caf-494f-dd29-e7dabc54ecbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                             int32\n",
              "pre_since_opened_mean        float32\n",
              "pre_since_opened_sum           int32\n",
              "pre_since_confirmed_mean     float32\n",
              "pre_since_confirmed_sum        int32\n",
              "                              ...   \n",
              "enc_loans_account_cur_sum      int32\n",
              "pclose_flag_mean             float32\n",
              "pclose_flag_sum                int32\n",
              "fclose_flag_mean             float32\n",
              "fclose_flag_sum                int32\n",
              "Length: 119, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_mean</th>\n",
              "      <td>float32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_sum</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_mean</th>\n",
              "      <td>float32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_sum</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enc_loans_account_cur_sum</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_mean</th>\n",
              "      <td>float32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_sum</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_mean</th>\n",
              "      <td>float32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_sum</th>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>119 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.memory_usage(deep=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "TADxwiFlQQiW",
        "outputId": "41c5cd6e-cbcb-4cdb-af7e-336a675c77b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index                        24000000\n",
              "id                           12000000\n",
              "pre_since_opened_mean        12000000\n",
              "pre_since_opened_sum         12000000\n",
              "pre_since_confirmed_mean     12000000\n",
              "                               ...   \n",
              "enc_loans_account_cur_sum    12000000\n",
              "pclose_flag_mean             12000000\n",
              "pclose_flag_sum              12000000\n",
              "fclose_flag_mean             12000000\n",
              "fclose_flag_sum              12000000\n",
              "Length: 120, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Index</th>\n",
              "      <td>24000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_mean</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_opened_sum</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre_since_confirmed_mean</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enc_loans_account_cur_sum</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_mean</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclose_flag_sum</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_mean</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fclose_flag_sum</th>\n",
              "      <td>12000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим теперь механизм кросс-валидации. Учитывая, что мы загружаем данные отдельными партациями, нам не подходит \"классический\" алгоритм разбиения датасета на train\\val выборки.\n",
        "\n",
        "Дело в том, что при разбиении каждой загружаемой партации на фолды \"на месте\" мы теряем репрезентативность, так как out-of-fold часть выборки будет локальной, а для адекватной валидации нужно, чтобы OOF охватывала весь датасет.\n",
        "\n",
        "Более того, без механизма согласования OOF по фолдам мы не сможем собрать аггрегированное предсказание наших моделей на тесте. Это связано с архитектурой решения и будет видно позже.\n",
        "\n",
        "Итак, рассмотрим теперь следующий механизм кросс-валидации:\n",
        "1. Создается датафрейм, содержащий id объектов всей (!) выборки. Учитывая, что это единственный int столбец, он не займет критически много места;\n",
        "2. Этот датафрейм расширяется колонкой fold, которая будет указывать, к валидации какого по счету фолда относится элемент;\n",
        "3. Колонка fold заполняется с использованием инструмента KFold из scikit_learn. Это стандартный инструмент для разбиения наборов данных на фолды;\n",
        "\n",
        "На этом этапе мы получаем датафрейм, глобально делящий все объекты из всех партаций в совокупности на фолды для кросс-валидации. Учитывая огромный объем каждой отдельной партации и случайность разбиения на фолды, мы получим примерно одинаковое соотношение объемов train и val частей в индивидуальных партациях.\n",
        "\n",
        "При обучении модели каждая партация загружается и рассматривается независимо от остальных. Используя полученный на этапе 3 датафрейм, из партации выбираются соответствующие текущему фолду строки. Для итогового случая создается отдельная модель. Она обучается на финальных данных и добавляется в общий ансамбль. Это гарантирует, что все модели ансамбля в совокупности будут обучены на полном объеме данных без исключения.  \n",
        "\n",
        "К концу обучения общее количество моделей в ансамбле будет равно MxN, по M моделей на каждую партацию, рассмотренную в N разных вариациях разбиения.\n",
        "\n",
        "Итоговое предсказание на тесте будет получено аггрегированием предсказаний всего ансамбля моделей. Именно на этом месте \"стреляет\" согласованность моделей в ансамбле."
      ],
      "metadata": {
        "id": "7_E0FDm4KXiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = {\n",
        "        os.path.join('./kaggle_data/data_for_competition/train_features', filename)\n",
        "        for filename in os.listdir('./kaggle_data/data_for_competition/train_features')\n",
        "}\n",
        "\n",
        "id_dfs = []\n",
        "for path in train_paths:\n",
        "    pf = ParquetFile(path)\n",
        "    df = pf.to_pandas(columns=['id'])\n",
        "    id_dfs.append(df)\n",
        "\n",
        "kf_ids = pd.concat(id_dfs, ignore_index=True)\n",
        "kf_ids['fold'] = -1\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=100, shuffle=True)\n",
        "for fold, (_, val_idx) in enumerate(kf.split(kf_ids)):\n",
        "    kf_ids.loc[val_idx, 'fold'] = fold"
      ],
      "metadata": {
        "id": "7XOh7fW9h5QW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf_ids.fold.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "aJvMUxMDKXvf",
        "outputId": "a5494e77-1d50-41d0-bc19-d5ab01e93b92"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fold\n",
              "1    600000\n",
              "4    600000\n",
              "2    600000\n",
              "0    600000\n",
              "3    600000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fold</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cols = list(train_data.columns.values)\n",
        "feat_cols.remove('id')"
      ],
      "metadata": {
        "id": "L-lleMFDK3am"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3YgCdUvLC30",
        "outputId": "a003f03e-08af-443d-910f-c22c9b9133f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pre_since_opened_mean',\n",
              " 'pre_since_opened_sum',\n",
              " 'pre_since_confirmed_mean',\n",
              " 'pre_since_confirmed_sum',\n",
              " 'pre_pterm_mean',\n",
              " 'pre_pterm_sum',\n",
              " 'pre_fterm_mean',\n",
              " 'pre_fterm_sum',\n",
              " 'pre_till_pclose_mean',\n",
              " 'pre_till_pclose_sum',\n",
              " 'pre_till_fclose_mean',\n",
              " 'pre_till_fclose_sum',\n",
              " 'pre_loans_credit_limit_mean',\n",
              " 'pre_loans_credit_limit_sum',\n",
              " 'pre_loans_next_pay_summ_mean',\n",
              " 'pre_loans_next_pay_summ_sum',\n",
              " 'pre_loans_outstanding_mean',\n",
              " 'pre_loans_outstanding_sum',\n",
              " 'pre_loans_total_overdue_mean',\n",
              " 'pre_loans_total_overdue_sum',\n",
              " 'pre_loans_max_overdue_sum_mean',\n",
              " 'pre_loans_max_overdue_sum_sum',\n",
              " 'pre_loans_credit_cost_rate_mean',\n",
              " 'pre_loans_credit_cost_rate_sum',\n",
              " 'pre_loans5_mean',\n",
              " 'pre_loans5_sum',\n",
              " 'pre_loans530_mean',\n",
              " 'pre_loans530_sum',\n",
              " 'pre_loans3060_mean',\n",
              " 'pre_loans3060_sum',\n",
              " 'pre_loans6090_mean',\n",
              " 'pre_loans6090_sum',\n",
              " 'pre_loans90_mean',\n",
              " 'pre_loans90_sum',\n",
              " 'is_zero_loans5_mean',\n",
              " 'is_zero_loans5_sum',\n",
              " 'is_zero_loans530_mean',\n",
              " 'is_zero_loans530_sum',\n",
              " 'is_zero_loans3060_mean',\n",
              " 'is_zero_loans3060_sum',\n",
              " 'is_zero_loans6090_mean',\n",
              " 'is_zero_loans6090_sum',\n",
              " 'is_zero_loans90_mean',\n",
              " 'is_zero_loans90_sum',\n",
              " 'pre_util_mean',\n",
              " 'pre_util_sum',\n",
              " 'pre_over2limit_mean',\n",
              " 'pre_over2limit_sum',\n",
              " 'pre_maxover2limit_mean',\n",
              " 'pre_maxover2limit_sum',\n",
              " 'is_zero_util_mean',\n",
              " 'is_zero_util_sum',\n",
              " 'is_zero_over2limit_mean',\n",
              " 'is_zero_over2limit_sum',\n",
              " 'is_zero_maxover2limit_mean',\n",
              " 'is_zero_maxover2limit_sum',\n",
              " 'enc_paym_0_mean',\n",
              " 'enc_paym_0_sum',\n",
              " 'enc_paym_1_mean',\n",
              " 'enc_paym_1_sum',\n",
              " 'enc_paym_2_mean',\n",
              " 'enc_paym_2_sum',\n",
              " 'enc_paym_3_mean',\n",
              " 'enc_paym_3_sum',\n",
              " 'enc_paym_4_mean',\n",
              " 'enc_paym_4_sum',\n",
              " 'enc_paym_5_mean',\n",
              " 'enc_paym_5_sum',\n",
              " 'enc_paym_6_mean',\n",
              " 'enc_paym_6_sum',\n",
              " 'enc_paym_7_mean',\n",
              " 'enc_paym_7_sum',\n",
              " 'enc_paym_8_mean',\n",
              " 'enc_paym_8_sum',\n",
              " 'enc_paym_9_mean',\n",
              " 'enc_paym_9_sum',\n",
              " 'enc_paym_10_mean',\n",
              " 'enc_paym_10_sum',\n",
              " 'enc_paym_11_mean',\n",
              " 'enc_paym_11_sum',\n",
              " 'enc_paym_12_mean',\n",
              " 'enc_paym_12_sum',\n",
              " 'enc_paym_13_mean',\n",
              " 'enc_paym_13_sum',\n",
              " 'enc_paym_14_mean',\n",
              " 'enc_paym_14_sum',\n",
              " 'enc_paym_15_mean',\n",
              " 'enc_paym_15_sum',\n",
              " 'enc_paym_16_mean',\n",
              " 'enc_paym_16_sum',\n",
              " 'enc_paym_17_mean',\n",
              " 'enc_paym_17_sum',\n",
              " 'enc_paym_18_mean',\n",
              " 'enc_paym_18_sum',\n",
              " 'enc_paym_19_mean',\n",
              " 'enc_paym_19_sum',\n",
              " 'enc_paym_20_mean',\n",
              " 'enc_paym_20_sum',\n",
              " 'enc_paym_21_mean',\n",
              " 'enc_paym_21_sum',\n",
              " 'enc_paym_22_mean',\n",
              " 'enc_paym_22_sum',\n",
              " 'enc_paym_23_mean',\n",
              " 'enc_paym_23_sum',\n",
              " 'enc_paym_24_mean',\n",
              " 'enc_paym_24_sum',\n",
              " 'enc_loans_account_holder_type_mean',\n",
              " 'enc_loans_account_holder_type_sum',\n",
              " 'enc_loans_credit_status_mean',\n",
              " 'enc_loans_credit_status_sum',\n",
              " 'enc_loans_credit_type_mean',\n",
              " 'enc_loans_credit_type_sum',\n",
              " 'enc_loans_account_cur_mean',\n",
              " 'enc_loans_account_cur_sum',\n",
              " 'pclose_flag_mean',\n",
              " 'pclose_flag_sum',\n",
              " 'fclose_flag_mean',\n",
              " 'fclose_flag_sum']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_data\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKttr7CKZTEs",
        "outputId": "03acc43d-280d-4768-c90d-affbd778b11e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подгружаем в память целевую переменную из train_target.csv:"
      ],
      "metadata": {
        "id": "i7w6SOfsIuvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_target = pd.read_csv('./kaggle_data/data_for_competition/train_target.csv')"
      ],
      "metadata": {
        "id": "bfLvn73m1IRR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "models = []\n",
        "\n",
        "tree_parameters = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 5,\n",
        "    'reg_lambda': 1,\n",
        "    'num_leaves': 64,\n",
        "    'n_estimators': 2000,\n",
        "    'min_data_in_leaf': 256,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "for fold_ in range(5):\n",
        "    print(f'Training with fold {fold_} started')\n",
        "\n",
        "    val_preds_list = []\n",
        "    val_targets_list = []\n",
        "\n",
        "    for path in train_paths:\n",
        "        pf = ParquetFile(path)\n",
        "        df = pf.to_pandas()\n",
        "        df = df.merge(kf_ids, on='id')\n",
        "        df = df.merge(train_target, on='id')\n",
        "\n",
        "        val = df[df['fold'] == fold]\n",
        "        train = df[df['fold'] != fold]\n",
        "\n",
        "        if val.empty or train.empty:\n",
        "            print('Skipped: no val or train data for this fold')\n",
        "            continue\n",
        "\n",
        "        features = [c for c in train.columns if c not in ('id', 'flag', 'fold')]\n",
        "\n",
        "        lgb_model = lgb.LGBMClassifier(**tree_parameters)\n",
        "        lgb_model.fit(\n",
        "            train[features], train.flag.values,\n",
        "            eval_set=[(val[features], val.flag.values)]\n",
        "        )\n",
        "\n",
        "        preds = lgb_model.predict_proba(val[features])[:, 1]\n",
        "        val_preds_list.append(preds)\n",
        "        val_targets_list.append(val['flag'].values)\n",
        "        models.append(lgb_model)\n",
        "\n",
        "        del df, val, train\n",
        "        gc.collect()\n",
        "\n",
        "    fold_preds = np.concatenate(val_preds_list)\n",
        "    fold_targets = np.concatenate(val_targets_list)\n",
        "    print('CV ROC-AUC: ', roc_auc_score(fold_targets, fold_preds))\n",
        "\n",
        "    print(f'Training with fold {fold_} completed')"
      ],
      "metadata": {
        "id": "aP_HGWrJHmZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь проделаем то же самое для тестовой выборки:"
      ],
      "metadata": {
        "id": "BV-4LQ6IIqc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./kaggle_data/data_for_competition/test_features"
      ],
      "metadata": {
        "id": "8lV3r7xL5oA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = aggregator.transform(\n",
        "    './kaggle_data/data_for_competition/test_data',\n",
        "    num_parts_to_preprocess_at_once=2,\n",
        "    num_parts_total=2,\n",
        "    save_to_path='./kaggle_data/data_for_competition/test_features',\n",
        "    verbose=True)\n",
        "\n",
        "for col in test_data.select_dtypes(include=['int64']).columns:\n",
        "    test_data[col] = pd.to_numeric(test_data[col], downcast='integer')\n",
        "\n",
        "for col in test_data.select_dtypes(include=['float64']).columns:\n",
        "    test_data[col] = test_data[col].astype(np.float32)"
      ],
      "metadata": {
        "id": "3EojSfui5nRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target = pd.read_csv('./kaggle_data/data_for_competition/test_target.csv')\n",
        "\n",
        "score = np.zeros(len(test_data))\n",
        "for model in tqdm(models):\n",
        "    score += model.predict_proba(test_data[feat_cols])[:, 1]/len(models)\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'].values,\n",
        "    'score': score\n",
        "})\n",
        "\n",
        "print('TEST ROC-AUC: ', roc_auc_score(test_target, score))"
      ],
      "metadata": {
        "id": "SYtybXw0Xq6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}